{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Resolution\n",
    " Submitted by : Sanketh Nagarajan (sn2692)\n",
    " \n",
    " User Name in Leaderboard : SankethNagarajan\n",
    " \n",
    " Number of team members : 1 (Solo participant)\n",
    " \n",
    " Email ID: sn2692@columbia.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading the given files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading Files\n",
    "amazon = np.asarray(pd.read_csv(\"amazon.csv\", encoding = \"ISO-8859-1\"))\n",
    "rotten_tomatoes = np.asarray(pd.read_csv(\"rotten_tomatoes.csv\", encoding = \"ISO-8859-1\"))\n",
    "train = np.asarray(pd.read_csv(\"train.csv\", encoding = \"ISO-8859-1\"))\n",
    "test = np.asarray(pd.read_csv(\"test.csv\", encoding = \"ISO-8859-1\"))\n",
    "holdout = np.asarray(pd.read_csv(\"holdout.csv\", encoding = \"ISO-8859-1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On inspection, amazon.csv has date values in place of the runtime column for some entries (with runtimes given in the \"star\" column). We correct them next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Processing Rows which have date values in Run time (for amazon)\n",
    "index = []\n",
    "b = amazon[:,1]\n",
    "for i in range(0,amazon.shape[0]):\n",
    "    if(\"/\" in str(b[i])):\n",
    "        index.append(i)\n",
    "#Replacing dates with run time & deleting runtime from star name (now empty valued)\n",
    "amazon1 = np.copy(amazon)\n",
    "amazon1[index,1] = amazon1[index,3]\n",
    "amazon1[index,3] = \"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the unwanted columns from both the movie datasets (like \"remarks\", \"year\", etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Deleting unwanted columns in amazon & rotten_tomatoes\n",
    "amazon1 = amazon1[:,0:4]\n",
    "rotten_tomatoes1 = np.copy(rotten_tomatoes[:,0:10])\n",
    "rotten_tomatoes1 = np.delete(rotten_tomatoes1,3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"runtime\" field has values as strings. Lets convert them into seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting Runtime to seconds & Int data type\n",
    "#For Amazon\n",
    "b = amazon1[:,1]\n",
    "amazon2 = np.copy(amazon1)\n",
    "for i in range(0,amazon1.shape[0]):\n",
    "    s = 0\n",
    "    if(str(b[i]) != 'nan'):\n",
    "        x = str(b[i]).split(\",\")\n",
    "        for j in range(0,len(x)):\n",
    "            k = str(x[j]).strip()\n",
    "            l = k.split(\" \")\n",
    "            if (\"hour\" in str(l[1])):\n",
    "                m = (int(l[0]) * 60 * 60)\n",
    "                s = s + m\n",
    "            if (\"min\" in str(l[1])):\n",
    "                s = s + (int(l[0]) * 60)\n",
    "            if (\"sec\" in  str(l[1])):\n",
    "                s = s + int(l[0])\n",
    "    amazon2[i,1] = int(s) \n",
    "\n",
    "#For Rotten Tomatoes\n",
    "b = rotten_tomatoes1[:,1]\n",
    "rotten_tomatoes2 = np.copy(rotten_tomatoes1)\n",
    "for i in range(0,len(b)):\n",
    "    s = 0\n",
    "    if (str(b[i])!='nan'):\n",
    "        a = rotten_tomatoes2[i,1]\n",
    "        a = str(a)[:-1]\n",
    "        c = a.split(\".\")\n",
    "        for j in range(0,len(c)):\n",
    "            k = str(c[j]).strip()\n",
    "            l = k.split(\" \")\n",
    "            if (\"hr\" in str(l[1])):\n",
    "                m = (int(l[0]) * 60 * 60)\n",
    "                s = s + m\n",
    "            if (\"min\" in str(l[1])):\n",
    "                s = s + (int(l[0]) * 60)\n",
    "    rotten_tomatoes2[i,1] = int(s)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observing the matching entries in the train.csv dataset, 3 features were found to be influential in the classification.\n",
    "\n",
    "Now we define 4 functions to calculate 3 features (definition given below) that will be used to training our machine learning classifier.\n",
    "\n",
    "Feature 1 : Gives the absolute difference between movie runtimes\n",
    "\n",
    "Feature 2 : String similarity between director names (out of 1)\n",
    "\n",
    "Feature 3: String similarity score between movie stars. It is calculated as follows:\n",
    "- Calculate similarity scores for each star in the \"star\" column of amazon.csv with the 6 other \"star\" columns in rotten tomatoes.\n",
    "- Select the highest score amongst all the comparisons and multiply it by the number of stars in the amazon.csv datastet for that movie. This is the third feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to claculate similarity score between 2 strings a & b\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "#Creating Feature1 (Absolute Difference between movie run times)\n",
    "def feature1(id1,id2):\n",
    "    ida = np.where(amazon2[:,0]==id1)[0][0]\n",
    "    idr = np.where(rotten_tomatoes2[:,0]==id2)[0][0]\n",
    "    return(abs(amazon2[ida,1] - rotten_tomatoes2[idr,1]))\n",
    "\n",
    "#Creating Feature2 (Similarity between Director Names)\n",
    "def feature2(id1,id2):\n",
    "    ida = np.where(amazon2[:,0]==id1)[0][0]\n",
    "    idr = np.where(rotten_tomatoes2[:,0]==id2)[0][0]\n",
    "    return(similar(amazon2[ida,2],rotten_tomatoes2[idr,2]))\n",
    "\n",
    "#Creating Feature3 (Similarity between Star names)\n",
    "def feature3(id1,id2):\n",
    "    ida = np.where(amazon2[:,0]==id1)[0][0]\n",
    "    idr = np.where(rotten_tomatoes2[:,0]==id2)[0][0] \n",
    "    a = str(amazon2[ida,3])\n",
    "    b = a.split(\",\")\n",
    "    r = rotten_tomatoes2[idr,3:9]\n",
    "    av = 0\n",
    "    h = 0\n",
    "    m = 0\n",
    "    for i in range(0,len(b)):\n",
    "        k = str(b[i]).strip()\n",
    "        if(k!='nan'):    \n",
    "            for j in range(0,len(r)):\n",
    "                l = str(r[j]).strip()\n",
    "                if(l!='nan'):\n",
    "                    s = similar(k,l)\n",
    "                    if(s > h):\n",
    "                        h = s\n",
    "            m = m + h\n",
    "    if (len(b) > 0):\n",
    "        av = m\n",
    "    \n",
    "    return(av)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us transform the train, test & holdout datasets according to the features we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Constructing the training data (according to the defined features)\n",
    "train_new = np.zeros((train.shape[0],4))\n",
    "for i in range(0,train.shape[0]):\n",
    "    id1 = train[i,0]\n",
    "    id2 = train[i,1]\n",
    "    res = train[i,2]\n",
    "    train_new[i,0] = feature1(id1,id2)\n",
    "    train_new[i,1] = feature2(id1,id2)\n",
    "    train_new[i,2] = feature3(id1,id2)\n",
    "    train_new[i,3] = res\n",
    "    \n",
    "#Constructing the testing data (according to the defined features)\n",
    "test_new = np.zeros((test.shape[0],3))\n",
    "for i in range(0,test.shape[0]):\n",
    "    id1 = test[i,0]\n",
    "    id2 = test[i,1]\n",
    "    test_new[i,0] = feature1(id1,id2)\n",
    "    test_new[i,1] = feature2(id1,id2)\n",
    "    test_new[i,2] = feature3(id1,id2)\n",
    "\n",
    "#Constructing the holdout data (according to the defined features)\n",
    "holdout_new = np.zeros((holdout.shape[0],3))\n",
    "for i in range(0,holdout.shape[0]):\n",
    "    id1 = holdout[i,0]\n",
    "    id2 = holdout[i,1]\n",
    "    holdout_new[i,0] = feature1(id1,id2)\n",
    "    holdout_new[i,1] = feature2(id1,id2)\n",
    "    holdout_new[i,2] = feature3(id1,id2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have modelled our entity resolution problem as a machine learning classification problem. The best classfier was found to be a Support Vector Machine Classifier with an \"rbf\" kernel. First, 10 fold cross validation was used to measure model accuracy. For evaluatory purposes we have split the training data into 2 sets (training & testing) to ensure there is no overfitting.\n",
    "\n",
    "From the training data we can see that the classes are imbalanced (there are only 28 mathcing movies in the train.csv dataset). I used Edited Nearest Neighbors as an undersampling technique to enable the model to learn more from matching examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.940614035088\n"
     ]
    }
   ],
   "source": [
    "X  = train_new[:,0:3]\n",
    "Y = train_new[:,3]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,stratify=Y,random_state=0) \n",
    "\n",
    "svc_pipe = make_pipeline(EditedNearestNeighbours(kind_sel=\"mode\", n_neighbors=5), StandardScaler(), SVC(kernel='rbf'))\n",
    "score = cross_val_score(svc_pipe, X_train, Y_train, cv=10)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average cross validation accuracy looks promising. Let's calcualte the accuracy on the test set which was split from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96825396825396826"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_pipe.fit(X_train,Y_train)\n",
    "svc_pipe.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is 96.8% accurate on our custom made test set. Let's now calculate the precision, recall & F1 score for the model fit on our whole train.csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "not matching       0.96      0.99      0.97       221\n",
      "    matching       0.86      0.64      0.73        28\n",
      "\n",
      " avg / total       0.95      0.95      0.94       249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_pipe.fit(X,Y)\n",
    "y_new = svc_pipe.predict(X)\n",
    "\n",
    "print(classification_report(Y, y_new,target_names=[\"not matching\", \"matching\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average precision and recall for each category is 0.95, whereas the average F1 score is 0.94\n",
    "\n",
    "The recall score for matching examples is a bit low (0.64) which can be attributed to the highly imbalanced training data.\n",
    "\n",
    "Since the results are good we now create the gold.csv file which contains predictions for test.csv examples & holdout_gold.csv file which contains predictions for holdout.csv examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_pipe.fit(X,Y)\n",
    "gold = svc_pipe.predict(test_new)\n",
    "d = {'gold':gold}\n",
    "out_test = pd.DataFrame(data=d)\n",
    "out_test.to_csv(\"gold.csv\", index=False)\n",
    "\n",
    "gold = svc_pipe.predict(holdout_new)\n",
    "d = {'gold':gold}\n",
    "out_test = pd.DataFrame(data=d)\n",
    "out_test.to_csv(\"holdout_gold.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair-wise comparison was avoided by building a model which can just compare the given pair of entities (by their ids) and tell with confidence whether they represent the same entity or not. \n",
    "\n",
    "Anyother technique like match scores between entities need to have a Cartesian product of both the datasets to decide which entity matches a given entity (by choosing the highest match score) in one of the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "not matching       0.96      0.99      0.97       221\n",
      "    matching       0.86      0.64      0.73        28\n",
      "\n",
      " avg / total       0.95      0.95      0.94       249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit = make_pipeline(EditedNearestNeighbours(kind_sel=\"mode\", n_neighbors=5), StandardScaler(), LogisticRegressionCV())\n",
    "logit.fit(X,Y)\n",
    "y_new = logit.predict(X)\n",
    "\n",
    "print(classification_report(Y, y_new,target_names=[\"not matching\", \"matching\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
